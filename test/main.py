import os
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer, util

# ========== ØªØºÛŒÛŒØ± Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Ø¯Ø±Ø³Øª project_root (Ø±Ø§Ù‡â€ŒØ­Ù„ Û±) ==========
# ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø§ÛŒÙ† Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¯Ø± ÙÙˆÙ„Ø¯Ø± project/test/ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯.
# os.path.dirname(__file__) -> ".../project/test"
# os.path.dirname(os.path.dirname(__file__)) -> ".../project"
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
# ================================================================

# Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ùˆ Ù…Ø¯Ù„â€ŒÙ‡Ø§
book_path       = os.path.join(project_root, "book.txt")
questions_path  = os.path.join(project_root, "test", "test.txt")
output_path     = os.path.join(project_root, "test", "result.txt")

mpnet_model_path = os.path.join(project_root, "models", "mpnet_local")
phi2_model_path  = os.path.join(project_root, "models", "phi-2")

# ====== Ù…Ø±Ø­Ù„Ù‡ 1: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ ======
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"==> Using device: {device}")

# Ù…Ø¯Ù„ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ø±Ø§ Ø±ÙˆÛŒ Ù‡Ù…Ø§Ù† Ø¯Ø³ØªÚ¯Ø§Ù‡ (CPU/GPU) ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
embedder = SentenceTransformer(mpnet_model_path, device=device)

# Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ (phi-2) Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ù…Ø±Ø¨ÙˆØ·Ù‡
tokenizer = AutoTokenizer.from_pretrained(phi2_model_path)
model     = AutoModelForCausalLM.from_pretrained(phi2_model_path).to(device)

# ====== Ù…Ø±Ø­Ù„Ù‡ 2: Ø®ÙˆØ§Ù†Ø¯Ù† Ú©ØªØ§Ø¨ Ùˆ Ø³Ø¤Ø§Ù„Ø§Øª ======
with open(book_path, "r", encoding="utf-8") as f:
    book_text = f.read()

with open(questions_path, "r", encoding="utf-8") as f:
    questions = [line.strip() for line in f if line.strip()]

# ====== Ù…Ø±Ø­Ù„Ù‡ 3: ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙ† Ø¨Ù‡ Ú†Ø§Ù†Ú© ======
def chunk_text(text, max_words=250, overlap=0.4):
    """
    Ù…ØªÙ† Ø¨Ù„Ù†Ø¯ Ø±Ø§ Ø¨Ù‡ ØªÚ©Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ø­Ø¯Ø§Ú©Ø«Ø± max_words Ú©Ù„Ù…Ù‡ ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    overlap Ø¯Ø±ØµØ¯ Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ Ø¨ÛŒÙ† Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§Ø³Øª (Ù…Ø«Ù„Ø§Ù‹ Ø§Ú¯Ø± overlap=0.4ØŒ
    Ù‡Ø± Ú†Ø§Ù†Ú© 40Ùª Ø§Ø² Ú†Ø§Ù†Ú© Ù‚Ø¨Ù„ÛŒ Ø±Ø§ ØªÚ©Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯).
    """
    words = text.split()
    step = int(max_words * (1 - overlap))
    if step < 1:
        step = max_words
    chunks = []
    for i in range(0, len(words), step):
        chunk = words[i : i + max_words]
        chunks.append(" ".join(chunk))
    return chunks

chunks = chunk_text(book_text, max_words=250, overlap=0.4)

print("==> Computing embeddings for all chunks â€¦")
chunk_embeddings = embedder.encode(
    chunks,
    convert_to_tensor=True,
    show_progress_bar=True
)
print("    âœ” Chunk embeddings computed.")

# ====== Ù…Ø±Ø­Ù„Ù‡ 4: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® ======
def answer_question(question, chunks, chunk_embeddings, top_n=5):
    """
    1) embedding Ø³Ø¤Ø§Ù„ Ø±Ø§ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯.
    2) Ø´Ø¨ÛŒÙ‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ Ø¨ÛŒÙ† embedding Ø³Ø¤Ø§Ù„ Ùˆ embedding Ù‡Ù…Ù‡Ù” Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    3) top_n Ú†Ø§Ù†Ú©Ù Ù…Ø±ØªØ¨Ø· Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    4) Ù‚Ø¨Ù„ Ø§Ø² Ú©Ù†Ø§Ø± Ù‡Ù…â€ŒÚ†ÛŒØ¯Ù†ØŒ Ù…Ø·Ù…Ø¦Ù† Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù…Ø¬Ù…ÙˆØ¹Ù‡Ù” Ù†Ù‡Ø§ÛŒÛŒÙ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ (prompt + context)
       Ø§Ø² Ø­Ø¯Ù 1024 ØªÙˆÚ©Ù† ÙØ±Ø§ØªØ± Ù†Ø±ÙˆØ¯.
    5) Ù¾Ø±Ø§Ù…Ù¾Øª Ø±Ø§ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ Ùˆ Ø¨Ù‡ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ ØªØ§ Ù¾Ø§Ø³Ø® Ø±Ø§ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†Ø¯.
    """
    # 4.1 â€“ embedding Ø³Ø¤Ø§Ù„
    q_embedding = embedder.encode(question, convert_to_tensor=True)

    # 4.2 â€“ Ù…Ø­Ø§Ø³Ø¨Ù‡Ù” Ø´Ø¨ÛŒÙ‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ top_n Ø§Ù†Ø¯ÛŒØ³
    cos_scores = util.cos_sim(q_embedding, chunk_embeddings)[0]
    top_indices = torch.topk(cos_scores, k=top_n).indices.tolist()

    # 4.3 â€“ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ùˆ Ø­Ø¬Ù… ØªÙˆÚ©Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ
    #   Ù‡Ø¯Ù Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù…Ø¬Ù…ÙˆØ¹ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ "best_chunks + Ø³Ø¤Ø§Ù„" Ø²ÛŒØ± 1024 Ø¨Ù…Ø§Ù†Ø¯.
    q_tokens = tokenizer.encode(f"Question: {question}\nAnswer:", add_special_tokens=False)
    q_token_count = len(q_tokens)
    max_input_tokens = 1024 - q_token_count - 20  # Û²Û° ØªÙˆÚ©Ù† Ø­Ø§Ø´ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®

    selected_chunks = []
    total_tokens = 0
    for idx in top_indices:
        chunk = chunks[idx]
        chunk_tok = tokenizer.encode(chunk, add_special_tokens=False)
        if total_tokens + len(chunk_tok) <= max_input_tokens:
            selected_chunks.append(chunk)
            total_tokens += len(chunk_tok)
        if total_tokens >= max_input_tokens:
            break

    if not selected_chunks:
        selected_chunks = [chunks[top_indices[0]]]

    best_chunks = "\n---\n".join(selected_chunks)

    # 4.4 â€“ Ø³Ø§Ø®Øª Ù¾Ø±Ø§Ù…Ù¾Øª Ù†Ù‡Ø§ÛŒÛŒ
    prompt = (
    "As a specialist in philosophy of mind, analyze the following question using the relevant information below. "
    "Write a self-contained, precise, and academic answer. Do not repeat the question or instructions.\n\n"
    "Context:\n"
    f"{best_chunks}\n\n"
    f"Question:\n{question}\n\n"
    "Answer:"
)


    # 4.5 â€“ ØªÙˆÚ©Ù†ÛŒØ²Ù‡ Ùˆ Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ device
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=1024,
        add_special_tokens=True
    ).to(device)

    # 4.6 â€“ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ try/except ØªØ§ Ø¯Ø± ØµÙˆØ±Øª Ø¨Ø±ÙˆØ² Ø®Ø·Ø§ÛŒ CUDAØŒ Ú©Ù„ Ø­Ù„Ù‚Ù‡ Ù‚Ø·Ø¹ Ù†Ø´ÙˆØ¯
    try:
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=250,
                temperature=0.6,
                top_p=0.85,
                repetition_penalty=1.3,
                pad_token_id=tokenizer.eos_token_id,
                early_stopping=True
            )
    except Exception as e:
        print(f"    [!] Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ø³Ø¤Ø§Ù„: {e}")
        return "(Error: could not generate answer)"

    # 4.7 â€“ Ø¯ÛŒÚ©Ø¯ Ø®Ø±ÙˆØ¬ÛŒ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø®Ø´ Answer:
    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "Answer:" in output_text:
        answer = output_text.split("Answer:")[-1].strip()
    else:
        answer = output_text.strip()

    if len(answer) < 20:
        return "(No clear answer generated â€” consider review.)"
    return answer

# ====== Ù…Ø±Ø­Ù„Ù‡ 5: ØªØ§Ø¨Ø¹ Ø§Ø³Ú©ÙˆØ±Ø¯Ù‡ÛŒ Ù¾Ø§Ø³Ø® ======
def score_answer(question, answer):
    """
    Ù…Ø¹ÛŒØ§Ø± Ø³Ø§Ø¯Ù‡: Ø´Ø¨ÛŒÙ‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³ÛŒÙ embedding Ø³Ø¤Ø§Ù„ Ùˆ embedding Ù¾Ø§Ø³Ø® Ø±Ø§
    Ø¨Ù‡ Ù…Ù‚ÛŒØ§Ø³ 0 ØªØ§ 100 ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    if answer.startswith("(Error") or answer.startswith("(No clear"):
        return 0
    q_emb = embedder.encode(question, convert_to_tensor=True)
    a_emb = embedder.encode(answer,   convert_to_tensor=True)
    sim = util.cos_sim(q_emb, a_emb).item()
    score = round(((sim + 1) / 2) * 100)
    return max(0, min(score, 100))

# ====== Ù…Ø±Ø­Ù„Ù‡ 6: Ø§Ø¬Ø±Ø§ÛŒ Ú©Ù„ÛŒ Ùˆ Ù†ÙˆØ´ØªÙ† Ø®Ø±ÙˆØ¬ÛŒ ======
with open(output_path, "w", encoding="utf-8") as f_out:
    f_out.write("=== Semantic QA Results ===\n\n")
    for i, question in enumerate(questions, start=1):
        print(f"({i}/{len(questions)}) ğŸ” Question: {question}")
        start_time = time.time()

        answer = answer_question(question, chunks, chunk_embeddings, top_n=5)
        elapsed_time = time.time() - start_time
        score = score_answer(question, answer)

        print(f"    âœ… Score: {score}/100   â±ï¸ Time: {elapsed_time:.2f}s")
        print(f"    â–¶ï¸ Answer preview: {answer[:80].replace(chr(10), ' ')}...\n")

        f_out.write(f"Question {i}: {question}\n")
        f_out.write(f"Answer: {answer}\n")
        f_out.write(f"Score: {score}/100\n")
        f_out.write(f"Time: {elapsed_time:.2f} seconds\n")
        f_out.write("-" * 40 + "\n\n")

print("âœ… All questions answered, scored, and saved to:", output_path)
